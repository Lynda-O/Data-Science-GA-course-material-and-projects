{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Clustering\n",
    "\n",
    "_Author: Sinan Uozdemir (San Francisco)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Know the difference between supervised and unsupervised learning.\n",
    "- Understand and know how to apply k-means clustering.\n",
    "- Understand and know how to apply density-based clustering (DBSCAN).\n",
    "- Define the Silhouette Coefficient and how it relates to clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Unsupervised Learning](#unsupervised-learning)\n",
    "\t- [Unsupervised Learning Example: Coin Clustering](#unsupervised-learning-example-coin-clustering)\n",
    "\t- [Common Types of Unsupervised Learning](#common-types-of-unsupervised-learning)\n",
    "\t- [Using Multiple Types of Learning Together](#using-multiple-types-of-learning-together)\n",
    "- [Clustering](#clustering)\n",
    "- [K-Means: Centroid Clustering](#k-means-centroid-clustering)\n",
    "\t- [Visual Demo](#visual-demo)\n",
    "\t- [K-Means Assumptions](#assumptions-are-important-k-means-assumes)\n",
    "- [K-Means Demo](#k-means-demo)\n",
    "\t- [K-Means Clustering](#k-means-clustering)\n",
    "\t- [Repeat With Scaled Data](#repeat-with-scaled-data)\n",
    "- [DBSCAN: Density-Based Clustering](#dbscan-density-based-clustering)\n",
    "\t- [Visual Demo](#visual-demo)\n",
    "- [DBSCAN Clustering Demo](#dbscan-clustering-demo)\n",
    "- [Hierarchical Clustering](#hierarchical-clustering)\n",
    "- [Clustering Metrics](#clustering-metrics)\n",
    "- [Clustering, Classification, and Regression](#clustering-classification-and-regression)\n",
    "- [Comparing Clustering Algorithms](#comparing-clustering-algorithms)\n",
    "- [Lesson Summary](#lesson-summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unsupervised-learning\"></a>\n",
    "## Unsupervised Learning\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Supervised learning** focuses on finding a relationship between a matrix of features and a response variable. So far, we have primarily studied supervised algorithms: Each observation (row of data) comes with one or more labels -- either categorical variables (classes) or measurements (regression).\n",
    "\n",
    "**Unsupervised learning** has a different goal: **feature discovery**. Unsupervised learning **extracts structure from data**. For example, you could segment grocery-store shoppers into \"clusters\" of shoppers who exhibit similar behaviors.\n",
    "\n",
    "There is typically **additional (latent) structure** hiding in the feature matrix. For example, some features might be related to each other or even redundant. There also could be groups of observations that seem to be related.\n",
    "\n",
    "\n",
    "<a id=\"common-types-of-unsupervised-learning\"></a>\n",
    "**Common Types of Unsupervised Learning**\n",
    "\n",
    "_Clustering_: Group “similar” data points together into meaningful groups.\n",
    "\n",
    "_Dimensionality Reduction_: Reduce the dimensionality of a data set by extracting features that capture most of the variance in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With unsupervised learning:\n",
    "\n",
    "- There's no clear objective.\n",
    "- There's no \"right answer\" (which means it's hard to tell how well you're doing).\n",
    "- There's no response variable — only observations with features.\n",
    "- Labeled data is not required."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"unsupervised-learning-example-coin-clustering\"></a>\n",
    "**An Example of Unsupervised Learning: Coin Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Observations: Coins\n",
    "- Features: Size and mass\n",
    "- Response: None (no hand-labeling required!)\n",
    "\n",
    "- Perform unsupervised learning:\n",
    "  - Cluster the coins based on “similarity.”\n",
    "  - You’re done!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"using-multiple-types-of-learning-together\"></a>\n",
    "**Using Multiple Types of Learning Together**\n",
    "\n",
    "None of these techniques are mutually exclusive, and they can be used in combination for better results. A useful example is **transfer learning**.\n",
    "\n",
    "Imagine you have a 100,000-row data set with no response values. Your job is to create a response with hand-labels and then create an algorithm to predict the response using supervised learning. \n",
    "\n",
    "Unfortunately, you only have time to label 10,000 rows. Does that mean the rest of the data is useless?\n",
    "\n",
    "The extra 90,000 rows are very useful! We can first use unsupervised learning to identify hidden structures such as related features and groups that naturally form in our data. The unsupervised learning algorithms can **transform both our unlabeled and labeled data into a form that's easier to predict from.**\n",
    "\n",
    "We then **use the new transformed data with supervised learning to get the predictions we were looking for**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering\"></a>\n",
    "## Clustering\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Helpful uses for clustering** \n",
    "   - Find items with similar behavior (users, products, voters, etc)\n",
    "   - Market segmentation\n",
    "   - Understand complex systems\n",
    "   - Discover meaningful categories for your data\n",
    "   - Reduce the number of classes by grouping (e.g. bourbons, scotches -> whiskeys)\n",
    "   - Reduce the dimensions of your problem\n",
    "   - Pre-processing! Create labels for supervised learning\n",
    "\n",
    "We're going to cover **three major clustering approaches**:\n",
    "\n",
    "- **Centroid clustering using k-means:** Looks for the centers of k pre-specified groups.\n",
    "\n",
    "- **Density-based clustering using DBSCAN:** Looks at gaps, or lack thereof, between datapoints.\n",
    "\n",
    "- **Hierarchical clustering using agglomerative clustering:** Forms groups of groups of groups in a hierarchy to determine clusters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-means-centroid-clustering\"></a>\n",
    "## K-Means: Centroid Clustering\n",
    "---\n",
    "\n",
    "K-means clustering is a popular centroid-based clustering algorithm. Similarly to k-nearest neighbors, this **partitions the entire space into regions (Voronoi partitions)**. In k-means clustering, **k** refers to the **number of clusters**. Also, since this is **unsupervised learning**, the regions are determined by the k-means algorithm instead of being provided by the training data.\n",
    "\n",
    "**Question:** Why might data often appear in centered clusters?\n",
    "\n",
    "![](./assets/images/clustering-centroids.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " In k-means clustering, we find $k$ clusters (where $k$ is user-specified), each **distributed around a single point** (called a **centroid**, an imaginary \"center point\" or the cluster's \"center of mass\").\n",
    "\n",
    "> **K-means seeks to minimize the sum of squares of each point about its cluster centroid.**\n",
    "\n",
    "If we manage to minimize this, then we claim to have found good clusters.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**K-means seeks to minimize the sum of squares of each point about its cluster centroid.**\n",
    " \n",
    "Let's see step-by-step how we might write an error function that describes this goal. Luckily, we already have experience with sum of squares, so we'll start there. \n",
    "\n",
    "> Although the final equation might look complex, you can understand it by thinking about it step-by-step and always relating each step back to the original goal. Do not let any fear of math get in your way!\n",
    "\n",
    "Keep in mind that unlike a lot of what you do in math, the math we're about to see is not a \"law\" and it is not \"derived\" from any equation. It was arrived at only because someone thought the goal metric above would be a useful description of one type of cluster. As we showed earlier, many other definitions of \"clusters\" are equally valid.\n",
    "\n",
    "#### Step One: Definitions\n",
    "\n",
    "Let's step through a few definitions we'll need to formalize the statement. Note that every data point must belong to exactly one cluster.\n",
    "\n",
    "- $S_i$ is the set of points in the $i$th cluster. For example:\n",
    "    - $S_1 = \\{(1, 1), (2, 1), (0, 1)\\}$.\n",
    "    - $S_2 = \\{(10, 10), (12, 10)\\}$.\n",
    "\n",
    "\n",
    "- $x$ is an arbitrary point. For example:\n",
    "    - $x = (2, 1)$.\n",
    "    - Note that $x \\in S_1$. This reads: \"$x$ belongs to $S_1$\" or just \"$x$ in $S_1$.\"\n",
    "    \n",
    "\n",
    "- $\\mu_i$ is the centroid (\"center point\") of all points in $S_i$. For example:\n",
    "    - $\\mu_1 = (\\frac{1+2+0}{3}, \\frac{1+1+1}{3}) = (1, 1)$.\n",
    "    - $\\mu_2 = (\\frac{10+12}{2}, \\frac{10+10}{2}) = (11, 10)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Two: Error of one cluster\n",
    "\n",
    "We need to **measure the \"tightness\" of each cluster** -- the closer its points are to the centroid, the better. So, we'll measure how far away each point is from the centroid. Further, we'll square each distance to particularly penalize far away points.\n",
    "\n",
    "So, the sum of the distances of each point $x$ to $\\mu$ is just:\n",
    "\n",
    "$$E_i(S) = {\\sum_{x \\in S} {\\|x - \\mu\\|^2}}$$\n",
    "\n",
    "> This is read: \"The sum of the square distances of each point in S to the centroid of S.\"\n",
    "\n",
    "**Question:** How does this relate to the goal statement?\n",
    "\n",
    "**Answer:** ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Three: Sum of all cluster errors\n",
    "\n",
    "Now, let's find this sum for each cluster. If we sum these sums together, that is the total error for all $k$ clusters:\n",
    "\n",
    "$$E_{total}(S_1, ..., S_k) = \\sum_{i=1}^k E_i(S_i)$$\n",
    "\n",
    "$$= \\sum_{i=1}^k {\\sum_{x \\in S} {\\|x - \\mu\\|^2}}$$\n",
    "\n",
    "**Question:** How does this relate to the goal statement?\n",
    "\n",
    "**Answer:** ---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step Four: Find the clusters that minimize total error\n",
    "\n",
    "Precisely, find $k$ partitions $S_1, …, S_k$ of the data with centroids $\\mu_1, …, \\mu_k$ that minimize $E_{total}$. In other words:\n",
    "\n",
    "$$\\text{argmin}_{S_1, …, S_k} \\sum_{i=1}^k {\\sum_{x \\in S_i} {\\|x - \\mu_i\\|^2}}$$\n",
    "\n",
    "> $\\text{argmin}_{S_1, …, S_k}\\ f(S_1, ..., S_k)$: Find the values of $S_1, ..., S_k$ that minimize $f(S_1, ..., S_k)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a computationally difficult problem to solve, so we often rely on heuristics (practical method that might not always be optimal).\n",
    "\n",
    "The \"standard\" heuristic is called **Lloyd’s Algorithm**:\n",
    "1. Start with $k$ initial (random) points* (we'll call these \"centroids\").\n",
    "2. Assign each datapoint to a cluster by finding its \"closest\" centroid (e.g. using Euclidean distance).\n",
    "3. Calculate new centroids based on the datapoints assigned to each cluster.\n",
    "4. Repeat 2-4 until clusters do not change.\n",
    "\n",
    "\\* There are a number of techniques for choosing initial points. For example, see the `k-means++` technique.\n",
    "\n",
    "**K Means is a powerful algorithm, but different starting points may give you different clusters. You won't necessarily get an optimal cluster.  **"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visual-demo\"></a>\n",
    "### Visual Demo\n",
    "\n",
    "[Click through](https://www.naftaliharris.com/blog/visualizing-k-means-clustering/) for a demo of k-means clustering in action.\n",
    "\n",
    "![voronoi](assets/voronoi.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"assumptions-are-important-k-means-assumes\"></a>\n",
    "### K-Means Assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-means assumes:\n",
    "\n",
    "- **k** is the **correct number of clusters**.\n",
    "- The data is **isotropically distributed (circular/spherical distribution)**.\n",
    "- The **variance is the same for each variable**.\n",
    "- Clusters are **roughly the same size**.\n",
    "\n",
    "View these resources to see counterexamples/cases where assumptions are not met:\n",
    "- [Variance Explained](http://varianceexplained.org/r/kmeans-free-lunch/)\n",
    "- [Scikit-Learn](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_assumptions.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we choose k?\n",
    "\n",
    "Finding the correct k to use for k-means clustering is not a simple task.\n",
    "\n",
    "We do not have a ground-truth we can use, so there isn't necessarily a \"correct\" number of clusters. However, we can find metrics that try to quantify the quality of our groupings.\n",
    "\n",
    "Our **application is also an important consideration**. For example, during **customer segmentation we want clusters that are large enough to be targetable by the marketing team**. In that case, even if the most natural-looking clusters are small, we may try to group several of them together so that it makes financial sense to target those groups.\n",
    "\n",
    "**Common approaches include:**\n",
    "- Figuring out the correct number of clusters from previous experience.\n",
    "- Using the **elbow method** to find a number of clusters that no longer seems to improve a clustering metric by a noticeable degree.\n",
    "  - The **silhouette coefficient** is a commonly used measure.\n",
    "  - For an example, check out this [silhouette analysis](http://scikit-learn.org/stable/auto_examples/cluster/plot_kmeans_silhouette_analysis.html) documentation on sklearn.\n",
    "  - If we're using clustering to improve performance on a supervised learning problem, then we can use our usual methods to test predictions.\n",
    "  \n",
    "### Clustering metric\n",
    "**Silhouette Coefficient**: Keep in mind that \"good fit\" for clustering is often arbitrary. For example, scoring isometetry might not apply if most clusters are naturally arbitrary shapes. \n",
    "\n",
    "The Silhouette Coefficient gives a score for each sample individually. At a high level, it **compares the point's cohesion to its cluster against its separation from the nearest other cluster. Ideally, you want the point to be very nearby other points in its own cluster and very far points in the nearest other cluster.**\n",
    "\n",
    "Here is how the Silhouette Coefficient is measured. Keep in mind how this math definition compares to our high-level idea of a sample's **cohesion vs. separation**:\n",
    "\n",
    "$$\\frac {b - a} {max(a,b)}$$\n",
    "\n",
    "- $a$ is the **mean distance between a sample and all other points in the cluster**.\n",
    "\n",
    "- $b$ is the **mean distance between a sample and all other points in the nearest cluster**.\n",
    "\n",
    "The **coefficient ranges between 1 and -1**. The **larger** the coefficient, the **better** the clustering.\n",
    "\n",
    "To get a score for all clusters rather than for a particular point, we average over all points to judge the cluster algorithm.\n",
    "\n",
    "**It's tempting to \"tune\" k as we have in supervised learning:**\n",
    "  - If we are working on a supervised learning problem, then this is possible.\n",
    "  - If we are using clustering to explore our data, then tuning is of little benefit since we do not know precisely what we are looking for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-means-demo\"></a>\n",
    "## K-Means Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Beer data set\n",
    "import pandas as pd\n",
    "url = './data/beer.txt'\n",
    "beer = pd.read_csv(url, sep=' ')\n",
    "beer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How would you cluster these beers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define X.\n",
    "X = beer.drop('name', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happened to Y?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-means-clustering\"></a>\n",
    "### K-Means Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means with three clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "km = KMeans(n_clusters=3, random_state=1)\n",
    "km.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cluster labels and sort by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer['cluster'] = km.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Allow plots to appear in the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.size'] = 14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia_list = []\n",
    "k_list = []\n",
    "\n",
    "for k in range(1, 10):\n",
    "    km = KMeans(n_clusters=k, random_state=1)\n",
    "    km.fit(X)\n",
    "    k_list.append(k)\n",
    "    inertia_list.append(km.inertia_)\n",
    "    \n",
    "plt.plot(k_list, inertia_list);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "silhouette_score(X, beer['cluster'], metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km.cluster_centers_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate the mean of each feature for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the `DataFrame` of cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "centers = beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a \"colors\" array for plotting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "colors = np.array(['red', 'green', 'blue', 'yellow'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot of calories versus alcohol, colored by cluster (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(beer.calories, beer.alcohol, c=colors[beer.cluster], s=50);\n",
    "\n",
    "# Cluster centers, marked by \"+\"\n",
    "plt.scatter(centers.calories, centers.alcohol, linewidths=3, marker='+', s=300, c='black');\n",
    "\n",
    "# Add labels.\n",
    "plt.xlabel('calories')\n",
    "plt.ylabel('alcohol')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot matrix (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"repeat-with-scaled-data\"></a>\n",
    "### Repeat With Scaled Data\n",
    "\n",
    "Unscaled features cause most algorithms to put too much weight onto one feature. We can scale our data to make sure k-means accounts for all features.\n",
    "\n",
    "Remember that k-means is looking for isotropic groups, meaning that they disperse from the center in all directions evenly. \n",
    "\n",
    "There is more than one choice of **scaling method (min/max, z-score, log, etc.)**, but the best choice is the one that **makes your clusters isotropic**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Center and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means with three clusters on scaled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km = KMeans(n_clusters=3, random_state=1)\n",
    "km.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cluster labels and sort by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer['cluster'] = km.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What are the \"characteristics\" of each cluster?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot matrix of new cluster assignments (0=red, 1=green, 2=blue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dbscan-density-based-clustering\"></a>\n",
    "## DBSCAN: Density-Based Clustering\n",
    "---\n",
    "\n",
    "#### Density-Based Clustering\n",
    "\n",
    "In DBSCAN (Density-Based Spatial Clustering of Applications with Noise), clusters are **created from areas of high density**. This can lead to **irregularly shaped regions**. Also, many parts of space may not belong to any region.\n",
    "\n",
    "**Question:** Why might data often appear in density-based clusters?\n",
    "\n",
    "The main idea of DBSCAN is to group together closely packed points by identifying:\n",
    "- Core points\n",
    "- Reachable points\n",
    "- Outliers (not reachable)\n",
    "\n",
    "**Its two parameters are:**\n",
    "- `min_samples`: At least this many points are required inside a neighborhood to form a dense cluster.\n",
    "- `eps`: epsion. This is the radius of a neighborhood.\n",
    "\n",
    "**How does it work?** \n",
    "\n",
    "1. Choose a random unvisited data point.\n",
    "\n",
    "2. Find all points in its neighborhood (i.e. at most `eps` units away). Then:\n",
    "    - **If there are at least `min_samples` points in its neighborhood:** Add all points in the neighborhood to the current cluster. Mark them as unvisited if they have not been visited.\n",
    "    - **Otherwise:** Mark the current point as visited. If the point is not part of a cluster, label the point as noise and go to Step 1.\n",
    "    \n",
    "3. If another point in the current cluster is unvisited, choose another point in the cluster and go to Step 2. Otherwise, start a new cluster and go to Step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"visual-demo\"></a>\n",
    "### Visual Demo\n",
    "\n",
    "[Click through](https://www.naftaliharris.com/blog/visualizing-dbscan-clustering/) for a demo of DBSCAN in action."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DBSCAN advantages**:\n",
    "- Can find **arbitrarily shaped clusters**.\n",
    "- **Don’t have to specify number of clusters.**\n",
    "- **Excludes outliers** automatically.\n",
    "\n",
    "**DBSCAN disadvantages**:\n",
    "- Doesn’t work well when clusters are of **varying densities**.\n",
    "- Hard to choose parameters that work for all clusters.\n",
    "- Can be **hard to choose correct parameters**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How does DBSCAN differ from k-means?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"dbscan-clustering-demo\"></a>\n",
    "## DBSCAN Clustering Demo\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DBSCAN with eps=1 and min_samples=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "db = DBSCAN(eps=1, min_samples=3)\n",
    "db.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Save the cluster labels and sort by cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer['cluster'] = db.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Review the cluster centers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer.groupby('cluster').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scatter plot matrix of DBSCAN cluster assignments (0=red, 1=green, 2=blue, -1=yellow)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.plotting.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"hierarchical-clustering\"></a>\n",
    "## Hierarchical Clustering\n",
    "---\n",
    "Hierarchical clustering, like k-means clustering, is another common form of clustering analysis. With this type of clustering we seek to do exactly what the name suggests:\n",
    "\n",
    "- **Build hierarchies of clusters.**\n",
    "\n",
    "Below, we see a tree data structure that stores clusters of points:\n",
    "- Each node represents a cluster of one or more data points.\n",
    "- Each leaf represents a single data point.\n",
    "- The root is the cluster containing all data points.\n",
    "- Each parent combines its children's clusters to create a new (larger) cluster.\n",
    "\n",
    "**Question:** When might hierarchical clustering be useful?\n",
    "\n",
    "Once the **links** are determined, we can **display** them in what is called a **dendrogram** — a graph that displays all of these links in their hierarchical structure.\n",
    "\n",
    "**A simple algorithm we could use to generate this tree:**\n",
    "\n",
    "1. **Create a cluster for each point**, containing only that point. (Create all leaf nodes.)\n",
    "2. Choose the **two clusters with centroids closest to each other**.\n",
    "    - Combine the two clusters into a new cluster that replaces the two individual clusters. (Create a new parent node.)\n",
    "3. Repeat Step 2 until only one cluster remains.\n",
    "\n",
    "Essentially we form groups of groups of groups.\n",
    "\n",
    "![](./assets/images/hierarchical-clustering.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "agg = AgglomerativeClustering(n_clusters=4)\n",
    "agg.fit(X_scaled)\n",
    "labels = agg.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cluster labels and sort by cluster.\n",
    "beer['cluster'] = agg.labels_\n",
    "beer.sort_values('cluster')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plot matrix of DBSCAN cluster assignments (0=red, 1=green, 2=blue, -1=yellow)\n",
    "pd.plotting.scatter_matrix(X, c=colors[beer.cluster], figsize=(10,10), s=100);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"clustering-classification-and-regression\"></a>\n",
    "## Clustering, Classification, and Regression\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We **can use clustering to discover new features, then use those features for either classification or regression.**\n",
    "\n",
    "For classification, we could use clusters directly to classify new points.\n",
    "\n",
    "For regression, we could use a dummy variable for the clusters as a variable in our regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import random\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a function to plot the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def set_colors(labels, colors='rgbykcm'):\n",
    "    colored_labels = []\n",
    "    for label in labels:\n",
    "        colored_labels.append(colors[label])\n",
    "    return colored_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create some synthetic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "data = []\n",
    "dist = multivariate_normal(mean=[0, 0], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    p = list(dist.rvs())\n",
    "    data.append(dist.rvs())\n",
    "dist = multivariate_normal(mean=[1, 5], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    data.append(dist.rvs())\n",
    "dist = multivariate_normal(mean=[2, 10], cov=[[0.5, 0.5],[0,0.1]])\n",
    "for i in range(150):\n",
    "    data.append(dist.rvs())\n",
    "\n",
    "    \n",
    "df = pd.DataFrame(data, columns=[\"x\", \"y\"])\n",
    "df.head()\n",
    "plt.scatter(df['x'], df['y'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Find clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit a DBSCAN estimator.\n",
    "estimator = DBSCAN(eps=0.8, min_samples=10)\n",
    "X = df[[\"x\", \"y\"]]\n",
    "estimator.fit(X)\n",
    "# Clusters are given in the labels_ attribute.\n",
    "labels = estimator.labels_\n",
    "\n",
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add cluster labels back to the `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Note that -1 clusters are outliers.\n",
    "df[\"cluster\"] = labels\n",
    "df = pd.concat([df, pd.get_dummies(df['cluster'], prefix=\"cluster\")], axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fit a linear model with clusters included."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = df[[\"x\", \"cluster_0\", \"cluster_1\", \"cluster_2\"]]\n",
    "y = df['y']\n",
    "model.fit(X, y)\n",
    "\n",
    "print((model.score(X, y)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.scatter(df[\"x\"], model.predict(X), color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What happens if we don't include the clusters we estimated?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "X = df[[\"x\"]]\n",
    "y = df['y']\n",
    "model.fit(X, y)\n",
    "print((model.score(X, y)))\n",
    "\n",
    "colors = set_colors(labels)\n",
    "plt.scatter(df['x'], df['y'], c=colors)\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "\n",
    "plt.scatter(df[\"x\"], model.predict(X), color='black')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-clustering-algorithms\"></a>\n",
    "## Comparing Clustering Algorithms\n",
    "\n",
    "- K-means\n",
    "  - Finds cluster centers.\n",
    "  - Must choose the number of clusters.\n",
    "  - Assumes clusters are isotropic.\n",
    "- DBSCAN\n",
    "  - Inspects local density to find clusters.\n",
    "  - Better than k-means for anisotropic clusters.\n",
    "  - Capable of finding outliers.\n",
    "- Hierarchical clustering\n",
    "  - Finds clusters by forming groups of groups of groups of points.\n",
    "  - Hierarchical clustering works well for non-spherical clusters.\n",
    "  - May be computationally expensive.\n",
    "  - Guaranteed to converge to the same solution (no random initialization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"lesson-summary\"></a>\n",
    "## Lesson Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Supervised learning vs. unsupervised learning\n",
    "    - The main difference between the two is whether we use response labels.\n",
    "- K-means, DBSCAN, and hierarchical clustering\n",
    "  - Can you summarize how each algorithm roughly works?\n",
    "- The Silhouette Coefficient\n",
    "  - What does the silhouette coefficient measure?\n",
    "- Using clustering along with supervised learning\n",
    "  - Why would we expect predictive power to improve when we include clusters?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
